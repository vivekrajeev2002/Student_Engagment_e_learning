{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fer import FER\n",
    "import dlib\n",
    "from scipy.spatial import distance\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import time\n",
    "from scipy.spatial import distance as dist\n",
    "from PyQt5.QtCore import QTimer, Qt\n",
    "from PyQt5.QtGui import QImage, QPixmap, QFont\n",
    "import sys\n",
    "from PyQt5.QtWidgets import QApplication,QHBoxLayout, QMainWindow, QLabel, QPushButton, QLineEdit, QVBoxLayout, QWidget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_FACE_WIDTH = 140  # [mm]\n",
    "NOSE_TO_CAMERA_DISTANCE = 600  # [mm]\n",
    "DEFAULT_WEBCAM = 0\n",
    "TOTAL_BLINKS = 0\n",
    "EYE_AR_CONSEC_FRAMES = 2\n",
    "LEFT_EYE_IRIS = [474, 475, 476, 477]\n",
    "RIGHT_EYE_IRIS = [469, 470, 471, 472]\n",
    "LEFT_EYE_OUTER_CORNER = [33]\n",
    "LEFT_EYE_INNER_CORNER = [133]\n",
    "RIGHT_EYE_OUTER_CORNER = [362]\n",
    "RIGHT_EYE_INNER_CORNER = [263]\n",
    "RIGHT_EYE_POINTS = [33, 160, 159, 158, 133, 153, 145, 144]\n",
    "LEFT_EYE_POINTS = [362, 385, 386, 387, 263, 373, 374, 380]\n",
    "NOSE_TIP_INDEX = 4\n",
    "CHIN_INDEX = 152\n",
    "LEFT_EYE_LEFT_CORNER_INDEX = 33\n",
    "RIGHT_EYE_RIGHT_CORNER_INDEX = 263\n",
    "LEFT_MOUTH_CORNER_INDEX = 61\n",
    "RIGHT_MOUTH_CORNER_INDEX = 291\n",
    "MIN_DETECTION_CONFIDENCE = 0.8\n",
    "MIN_TRACKING_CONFIDENCE = 0.8\n",
    "MOVING_AVERAGE_WINDOW = 10\n",
    "initial_pitch, initial_yaw, initial_roll = None, None, None\n",
    "calibrated = False\n",
    "DEFAULT_WEBCAM = 0  # Default webcam number\n",
    "\n",
    "# Iris and eye corners landmarks indices\n",
    "LEFT_IRIS = [474, 475, 476, 477]\n",
    "RIGHT_IRIS = [469, 470, 471, 472]\n",
    "L_H_LEFT = [33]  # Left eye Left Corner\n",
    "L_H_RIGHT = [133]  # Left eye Right Corner\n",
    "R_H_LEFT = [362]  # Right eye Left Corner\n",
    "R_H_RIGHT = [263]  # Right eye Right Corner\n",
    "\n",
    "# Blinking Detection landmark's indices.\n",
    "# P0, P3, P4, P5, P8, P11, P12, P13\n",
    "RIGHT_EYE_POINTS = [33, 160, 159, 158, 133, 153, 145, 144]\n",
    "LEFT_EYE_POINTS = [362, 385, 386, 387, 263, 373, 374, 380]\n",
    "\n",
    "# Face Selected points indices for Head Pose Estimation\n",
    "_indices_pose = [1, 33, 61, 199, 263, 291]\n",
    "\n",
    "# Function to calculate vector position\n",
    "def vector_position(point1, point2):\n",
    "    x1, y1 = point1.ravel()\n",
    "    x2, y2 = point2.ravel()\n",
    "    return x2 - x1, y2 - y1\n",
    "\n",
    "\n",
    "def euclidean_distance_3D(points):\n",
    "    # Get the three points.\n",
    "    P0, P3, P4, P5, P8, P11, P12, P13 = points\n",
    "\n",
    "    # Calculate the numerator.\n",
    "    numerator = (\n",
    "        np.linalg.norm(P3 - P13) ** 3\n",
    "        + np.linalg.norm(P4 - P12) ** 3\n",
    "        + np.linalg.norm(P5 - P11) ** 3\n",
    "    )\n",
    "    # Calculate the denominator.\n",
    "    denominator = 3 * np.linalg.norm(P0 - P8) ** 3\n",
    "    # Calculate the distance.\n",
    "    distance = numerator / denominator\n",
    "    return distance\n",
    "\n",
    "# Scale factor based on user's face width (assumes model face width is 150mm)\n",
    "scale_factor = USER_FACE_WIDTH / 150.0\n",
    "# 3D model points.\n",
    "model_points = np.array([\n",
    "    (0.0, 0.0, 0.0),             # Nose tip\n",
    "    (0.0, -330.0 * scale_factor, -65.0 * scale_factor),        # Chin\n",
    "    (-225.0 * scale_factor, 170.0 * scale_factor, -135.0 * scale_factor),     # Left eye left corner\n",
    "    (225.0 * scale_factor, 170.0 * scale_factor, -135.0 * scale_factor),      # Right eye right corner\n",
    "    (-150.0 * scale_factor, -150.0 * scale_factor, -125.0 * scale_factor),    # Left Mouth corner\n",
    "    (150.0 * scale_factor, -150.0 * scale_factor, -125.0 * scale_factor)      # Right mouth corner\n",
    "])\n",
    "\n",
    "def estimate_head_pose(landmarks, image_size):\n",
    "    # Scale factor based on user's face width (assumes model face width is 150mm)\n",
    "    scale_factor = USER_FACE_WIDTH / 150.0\n",
    "    # 3D model points.\n",
    "    model_points = np.array([\n",
    "        (0.0, 0.0, 0.0),             # Nose tip\n",
    "        (0.0, -330.0 * scale_factor, -65.0 * scale_factor),        # Chin\n",
    "        (-225.0 * scale_factor, 170.0 * scale_factor, -135.0 * scale_factor),     # Left eye left corner\n",
    "        (225.0 * scale_factor, 170.0 * scale_factor, -135.0 * scale_factor),      # Right eye right corner\n",
    "        (-150.0 * scale_factor, -150.0 * scale_factor, -125.0 * scale_factor),    # Left Mouth corner\n",
    "        (150.0 * scale_factor, -150.0 * scale_factor, -125.0 * scale_factor)      # Right mouth corner\n",
    "    ])\n",
    "    \n",
    "\n",
    "    # Camera internals\n",
    "    focal_length = image_size[1]\n",
    "    center = (image_size[1]/2, image_size[0]/2)\n",
    "    camera_matrix = np.array(\n",
    "        [[focal_length, 0, center[0]],\n",
    "         [0, focal_length, center[1]],\n",
    "         [0, 0, 1]], dtype = \"double\"\n",
    "    )\n",
    "\n",
    "    # Assuming no lens distortion\n",
    "    dist_coeffs = np.zeros((4,1))\n",
    "\n",
    "    # 2D image points from landmarks, using defined indices\n",
    "    image_points = np.array([\n",
    "        landmarks[NOSE_TIP_INDEX],            # Nose tip\n",
    "        landmarks[CHIN_INDEX],                # Chin\n",
    "        landmarks[LEFT_EYE_LEFT_CORNER_INDEX],  # Left eye left corner\n",
    "        landmarks[RIGHT_EYE_RIGHT_CORNER_INDEX],  # Right eye right corner\n",
    "        landmarks[LEFT_MOUTH_CORNER_INDEX],      # Left mouth corner\n",
    "        landmarks[RIGHT_MOUTH_CORNER_INDEX]      # Right mouth corner\n",
    "    ], dtype=\"double\")\n",
    "\n",
    "\n",
    "        # Solve for pose\n",
    "    (success, rotation_vector, translation_vector) = cv.solvePnP(model_points, image_points, camera_matrix, dist_coeffs, flags=cv.SOLVEPNP_ITERATIVE)\n",
    "\n",
    "    # Convert rotation vector to rotation matrix\n",
    "    rotation_matrix, _ = cv.Rodrigues(rotation_vector)\n",
    "\n",
    "    # Combine rotation matrix and translation vector to form a 3x4 projection matrix\n",
    "    projection_matrix = np.hstack((rotation_matrix, translation_vector.reshape(-1, 1)))\n",
    "\n",
    "    # Decompose the projection matrix to extract Euler angles\n",
    "    _, _, _, _, _, _, euler_angles = cv.decomposeProjectionMatrix(projection_matrix)\n",
    "    pitch, yaw, roll = euler_angles.flatten()[:3]\n",
    "\n",
    "\n",
    "     # Normalize the pitch angle\n",
    "    pitch = normalize_pitch(pitch)\n",
    "\n",
    "    return pitch, yaw, roll\n",
    "\n",
    "def normalize_pitch(pitch):\n",
    "    \"\"\"\n",
    "    Normalize the pitch angle to be within the range of [-90, 90].\n",
    "\n",
    "    Args:\n",
    "        pitch (float): The raw pitch angle in degrees.\n",
    "\n",
    "    Returns:\n",
    "        float: The normalized pitch angle.\n",
    "    \"\"\"\n",
    "    # Map the pitch angle to the range [-180, 180]\n",
    "    if pitch > 180:\n",
    "        pitch -= 360\n",
    "\n",
    "    # Invert the pitch angle for intuitive up/down movement\n",
    "    pitch = -pitch\n",
    "\n",
    "    # Ensure that the pitch is within the range of [-90, 90]\n",
    "    if pitch < -90:\n",
    "        pitch = -(180 + pitch)\n",
    "    elif pitch > 90:\n",
    "        pitch = 180 - pitch\n",
    "        \n",
    "    pitch = -pitch\n",
    "\n",
    "    return pitch\n",
    "\n",
    "# This function calculates the blinking ratio of a person.\n",
    "def blinking_ratio(landmarks):\n",
    "    \"\"\"Calculates the blinking ratio of a person.\n",
    "\n",
    "    Args:\n",
    "        landmarks: A facial landmarks in 3D normalized.\n",
    "\n",
    "    Returns:\n",
    "        The blinking ratio of the person, between 0 and 1, where 0 is fully open and 1 is fully closed.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the right eye ratio.\n",
    "    right_eye_ratio = euclidean_distance_3D(landmarks[RIGHT_EYE_POINTS])\n",
    "\n",
    "    # Get the left eye ratio.\n",
    "    left_eye_ratio = euclidean_distance_3D(landmarks[LEFT_EYE_POINTS])\n",
    "\n",
    "    # Calculate the blinking ratio.\n",
    "    ratio = (right_eye_ratio + left_eye_ratio + 1) / 2\n",
    "\n",
    "    return ratio\n",
    "\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh.FaceMesh(\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=MIN_DETECTION_CONFIDENCE,\n",
    "    min_tracking_confidence=MIN_TRACKING_CONFIDENCE,\n",
    ")\n",
    "\n",
    "def poseDetector(out):\n",
    "    v=0\n",
    "    ENABLE_HEAD_POSE = True\n",
    "    SHOW_ON_SCREEN_DATA = True\n",
    "    TOTAL_BLINKS = 0\n",
    "    EYES_BLINK_FRAME_COUNTER = 0\n",
    "    BLINK_THRESHOLD = 0.51\n",
    "    EYE_AR_CONSEC_FRAMES = 2\n",
    "    LEFT_EYE_IRIS = [474, 475, 476, 477]\n",
    "    RIGHT_EYE_IRIS = [469, 470, 471, 472]\n",
    "    LEFT_EYE_OUTER_CORNER = [33]\n",
    "    LEFT_EYE_INNER_CORNER = [133]\n",
    "    RIGHT_EYE_OUTER_CORNER = [362]\n",
    "    RIGHT_EYE_INNER_CORNER = [263]\n",
    "    EYES_BLINK_FRAME_COUNTER = (\n",
    "        0  # Counts the number of consecutive frames with a potential blink\n",
    "    )\n",
    "    BLINK_THRESHOLD = 0.51  # Threshold for the eye aspect ratio to trigger a blink\n",
    "    EYE_AR_CONSEC_FRAMES = (\n",
    "        2  # Number of consecutive frames below the threshold to confirm a blink\n",
    "    )\n",
    "    frame = out.copy()         \n",
    "    rgb_frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "    img_h, img_w = frame.shape[:2]\n",
    "    results = mp_face_mesh.process(rgb_frame)\n",
    "    if results.multi_face_landmarks:\n",
    "        mesh_points = np.array(\n",
    "            [\n",
    "                np.multiply([p.x, p.y], [img_w, img_h]).astype(int)\n",
    "                for p in results.multi_face_landmarks[0].landmark\n",
    "            ]\n",
    "        )\n",
    "        mesh_points_3D = np.array(\n",
    "            [[n.x, n.y, n.z] for n in results.multi_face_landmarks[0].landmark]\n",
    "        )\n",
    "        # getting the head pose estimation 3d points\n",
    "        head_pose_points_3D = np.multiply(\n",
    "            mesh_points_3D[_indices_pose], [img_w, img_h, 1]\n",
    "        )\n",
    "        head_pose_points_2D = mesh_points[_indices_pose]\n",
    "\n",
    "        # collect nose three dimension and two dimension points\n",
    "        nose_3D_point = np.multiply(head_pose_points_3D[0], [1, 1, 3000])\n",
    "        nose_2D_point = head_pose_points_2D[0]\n",
    "\n",
    "        # create the camera matrix\n",
    "        focal_length = 1 * img_w\n",
    "\n",
    "        cam_matrix = np.array(\n",
    "            [[focal_length, 0, img_h / 2], [0, focal_length, img_w / 2], [0, 0, 1]]\n",
    "        )\n",
    "\n",
    "        # The distortion parameters\n",
    "        dist_matrix = np.zeros((4, 1), dtype=np.float64)\n",
    "\n",
    "        head_pose_points_2D = np.delete(head_pose_points_3D, 2, axis=1)\n",
    "        head_pose_points_3D = head_pose_points_3D.astype(np.float64)\n",
    "        head_pose_points_2D = head_pose_points_2D.astype(np.float64)\n",
    "        # Solve PnP\n",
    "        success, rot_vec, trans_vec = cv.solvePnP(\n",
    "            head_pose_points_3D, head_pose_points_2D, cam_matrix, dist_matrix\n",
    "        )\n",
    "        # Get rotational matrix\n",
    "        rotation_matrix, jac = cv.Rodrigues(rot_vec)\n",
    "\n",
    "        # Get angles\n",
    "        angles, mtxR, mtxQ, Qx, Qy, Qz = cv.RQDecomp3x3(rotation_matrix)\n",
    "\n",
    "        # Get the y rotation degree\n",
    "        angle_x = angles[0] * 360\n",
    "        angle_y = angles[1] * 360\n",
    "        z = angles[2] * 360\n",
    "\n",
    "        # if angle cross the values then\n",
    "        threshold_angle = 10\n",
    "        # See where the user's head tilting\n",
    "        if angle_y < -threshold_angle:\n",
    "            face_looks = \"Right\"\n",
    "        elif angle_y > threshold_angle:\n",
    "            face_looks = \"Left\"\n",
    "        elif angle_x < -threshold_angle:\n",
    "            face_looks = \"Down\"\n",
    "        elif angle_x > threshold_angle:\n",
    "            face_looks = \"Up\"\n",
    "        else:\n",
    "            face_looks = \"Forward\"\n",
    "            v=1\n",
    "        if face_looks == 'Forward':\n",
    "            face_looks='Listening '\n",
    "        else:\n",
    "            face_looks=\"Not Listening  (\"+face_looks+')'\n",
    "        if SHOW_ON_SCREEN_DATA:\n",
    "            cv.putText(\n",
    "                frame,\n",
    "                f\"Engagment :  {face_looks}\",\n",
    "                (img_w - 400, 80),\n",
    "                cv.FONT_HERSHEY_TRIPLEX,\n",
    "                0.6,\n",
    "                (0, 0, 0),\n",
    "                2,\n",
    "                cv.LINE_AA,\n",
    "            )\n",
    "        # Display the nose direction\n",
    "        nose_3d_projection, jacobian = cv.projectPoints(nose_3D_point, rot_vec, trans_vec, cam_matrix, dist_matrix)\n",
    "\n",
    "        p1 = nose_2D_point\n",
    "        p2 = (\n",
    "            int(nose_2D_point[0] + angle_y * 10),\n",
    "            int(nose_2D_point[1] - angle_x * 10),\n",
    "        )\n",
    "\n",
    "        cv.line(frame, p1, p2, (255, 0, 255), 3)\n",
    "        # getting the blinking ratio\n",
    "        eyes_aspect_ratio = blinking_ratio(mesh_points_3D)\n",
    "        \n",
    "        if eyes_aspect_ratio <= BLINK_THRESHOLD:\n",
    "            EYES_BLINK_FRAME_COUNTER += 1\n",
    "        # else check if eyes are closed is greater EYE_AR_CONSEC_FRAMES frame then\n",
    "        # count the this as a blink\n",
    "        # make frame counter equal to zero\n",
    "\n",
    "        else:\n",
    "            if EYES_BLINK_FRAME_COUNTER > EYE_AR_CONSEC_FRAMES:\n",
    "                TOTAL_BLINKS += 1\n",
    "            EYES_BLINK_FRAME_COUNTER = 0\n",
    "        \n",
    "        # Display all facial landmarks if enabled\n",
    "\n",
    "        for point in mesh_points:\n",
    "            cv.circle(frame, tuple(point), 1, (0, 255, 0), -1)\n",
    "        # Process and display eye features\n",
    "        \n",
    "        (l_cx, l_cy), l_radius = cv.minEnclosingCircle(mesh_points[LEFT_EYE_IRIS])\n",
    "        (r_cx, r_cy), r_radius = cv.minEnclosingCircle(mesh_points[RIGHT_EYE_IRIS])\n",
    "        center_left = np.array([l_cx, l_cy], dtype=np.int32)\n",
    "        center_right = np.array([r_cx, r_cy], dtype=np.int32)\n",
    "        \n",
    "        # Highlighting the irises and corners of the eyes\n",
    "        cv.circle(\n",
    "            frame, center_left, int(l_radius), (255, 0, 255), 2, cv.LINE_AA\n",
    "        )  # Left iris\n",
    "        cv.circle(\n",
    "            frame, center_right, int(r_radius), (255, 0, 255), 2, cv.LINE_AA\n",
    "        )  # Right iris\n",
    "        cv.circle(\n",
    "            frame, mesh_points[LEFT_EYE_INNER_CORNER][0], 3, (255, 255, 255), -1, cv.LINE_AA\n",
    "        )  # Left eye right corner\n",
    "        cv.circle(\n",
    "            frame, mesh_points[LEFT_EYE_OUTER_CORNER][0], 3, (0, 255, 255), -1, cv.LINE_AA\n",
    "        )  # Left eye left corner\n",
    "        cv.circle(\n",
    "            frame, mesh_points[RIGHT_EYE_INNER_CORNER][0], 3, (255, 255, 255), -1, cv.LINE_AA\n",
    "        )  # Right eye right corner\n",
    "        cv.circle(\n",
    "            frame, mesh_points[RIGHT_EYE_OUTER_CORNER][0], 3, (0, 255, 255), -1, cv.LINE_AA\n",
    "        )  # Right eye left corner\n",
    "        \n",
    "        # Calculating relative positions\n",
    "        l_dx, l_dy = vector_position(mesh_points[LEFT_EYE_OUTER_CORNER], center_left)\n",
    "        r_dx, r_dy = vector_position(mesh_points[RIGHT_EYE_OUTER_CORNER], center_right)\n",
    "        return v,frame\n",
    "    else:\n",
    "        return v,out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:c:\\Users\\vivek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:C:\\Users\\vivek\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#CALCULATE EYE STATUS\n",
    "def eye_aspect_ratio(eye):\n",
    "    # Compute the Euclidean distances between the two sets of vertical eye landmarks\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "\n",
    "    # Compute the Euclidean distance between the horizontal eye landmarks\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "\n",
    "    # Calculate the eye aspect ratio\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Load pre-trained facial landmarks predictor and detector\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")  # Download from dlib website\n",
    "\n",
    "# Indices for eyes landmarks in the 68-point model\n",
    "LEFT_EYE = slice(36, 42)\n",
    "RIGHT_EYE = slice(42, 48)\n",
    "\n",
    "# Threshold for EAR to detect closure\n",
    "EAR_THRESHOLD = 0.25\n",
    "\n",
    "def eyestatus(out):\n",
    "    frame=out.copy()\n",
    "    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "    # Detect faces\n",
    "    faces = detector(gray)\n",
    "    v1=0\n",
    "    for face in faces:\n",
    "        # Detect landmarks\n",
    "        landmarks = predictor(gray, face)\n",
    "\n",
    "        # Convert landmarks to a numpy array\n",
    "        landmarks_points = [(landmarks.part(n).x, landmarks.part(n).y) for n in range(68)]\n",
    "\n",
    "        # Extract left and right eye points\n",
    "        left_eye = landmarks_points[LEFT_EYE]\n",
    "        right_eye = landmarks_points[RIGHT_EYE]\n",
    "\n",
    "        # Calculate EAR for both eyes\n",
    "        left_ear = eye_aspect_ratio(left_eye)\n",
    "        right_ear = eye_aspect_ratio(right_eye)\n",
    "\n",
    "        # Average the EAR\n",
    "        avg_ear = (left_ear + right_ear) / 2.0\n",
    "        \n",
    "        # Detect eye closure\n",
    "        if avg_ear < EAR_THRESHOLD:\n",
    "            v1=0\n",
    "            cv.putText(frame, \"Eyes Closed\", (30, 30), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        else:\n",
    "            v1=2.5\n",
    "            cv.putText(frame, \"Eyes Open\", (30, 30), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        # Draw eyes on the frame for visualization\n",
    "        for point in left_eye + right_eye:\n",
    "            cv.circle(frame, point, 2, (0, 255, 0), -1)\n",
    "    \n",
    "    return v1,frame\n",
    "\n",
    "\n",
    "emotion_detector = FER()\n",
    "def emotioninvoke(fr):\n",
    "    v=0\n",
    "    frame=fr.copy()\n",
    "    emotions = emotion_detector.detect_emotions(frame)\n",
    "    if(len(emotions)==0):\n",
    "        return v,frame\n",
    "    # Loop through detected faces and their emotions\n",
    "    \n",
    "    for emotion_data in emotions:\n",
    "        box = emotion_data[\"box\"]  # Bounding box for the face\n",
    "        emotions = emotion_data[\"emotions\"]  # Dictionary of emotions\n",
    "\n",
    "        # Get the most likely emotion\n",
    "        top_emotion = max(emotions, key=emotions.get)\n",
    "\n",
    "        # Draw the bounding box\n",
    "        x, y, w, h = box\n",
    "        cv.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 255), 2)\n",
    "        \n",
    "        # Display the top emotion\n",
    "        label = f\"{top_emotion}: {emotions[top_emotion]:.2f}\"\n",
    "        cv.putText(frame, label, (x, y - 10), cv.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "        if(label[0] == 'a'):\n",
    "            v=0.1\n",
    "        elif(label[0:2] == 'sa'):\n",
    "            v=0.3\n",
    "        elif(label[0] == 'f'):\n",
    "            v=0.5\n",
    "        elif(label[0] == 's'):\n",
    "            v=0.7\n",
    "        elif(label[0] == 'd'):\n",
    "            v=0.9\n",
    "        elif(label[0] == 'h'):\n",
    "            v=1.1\n",
    "        elif(label[0] == 'n'):\n",
    "            v = 1.4\n",
    "            \n",
    "        return v,frame\n",
    "\n",
    "\n",
    "\n",
    "class VideoWindow(QWidget):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ls = []\n",
    "        self.curr_time = time.time()\n",
    "        self.last_time = time.time()\n",
    "        self.setWindowTitle(\"OpenCV Output with Filters\")\n",
    "        self.setGeometry(100, 100, 1200, 800)\n",
    "        self.setStyleSheet(\"background-color: purple;\")  # Set purple background color\n",
    "\n",
    "        # Layout to hold four labels in a row\n",
    "        self.video_layout = QHBoxLayout()\n",
    "        self.labels = [QLabel() for _ in range(4)]\n",
    "        for label in self.labels:\n",
    "            label.setFixedSize(400, 370)  # Increase the size of video frames\n",
    "            label.setStyleSheet(\"background-color: black;\")  # Add background for labels\n",
    "            self.video_layout.addWidget(label)\n",
    "\n",
    "        # Label to display the average of `ls`\n",
    "        self.avg_label = QLabel(\"Average: Calculating...\")\n",
    "        self.avg_label.setAlignment(Qt.AlignCenter)\n",
    "        self.avg_label.setStyleSheet(\n",
    "            \"font-size: 18px; font-weight: bold; color: white; background-color: navy; padding: 10px;\"\n",
    "        )\n",
    "\n",
    "        # Stop button\n",
    "        self.stop_button = QPushButton(\"Stop\")\n",
    "        self.stop_button.setStyleSheet(\n",
    "            \"font-size: 18px; padding: 10px; background-color: red; color: white;\"\n",
    "        )\n",
    "        self.stop_button.clicked.connect(self.stop_video)\n",
    "\n",
    "        # Main layout\n",
    "        self.main_layout = QVBoxLayout()\n",
    "        self.main_layout.addLayout(self.video_layout)\n",
    "        self.main_layout.addWidget(self.avg_label, alignment=Qt.AlignCenter)\n",
    "        self.main_layout.addWidget(self.stop_button, alignment=Qt.AlignCenter)\n",
    "        self.setLayout(self.main_layout)\n",
    "\n",
    "        # OpenCV capture (video feed)\n",
    "        self.capture = cv.VideoCapture(0)\n",
    "\n",
    "        # Timer to periodically update the feed\n",
    "        self.timer = QTimer(self)\n",
    "        self.timer.timeout.connect(self.update_video_stream)\n",
    "        self.timer.start(100)\n",
    "\n",
    "    def update_video_stream(self):\n",
    "        ret, frame = self.capture.read()\n",
    "        if ret:\n",
    "            self.curr_time = time.time()\n",
    "\n",
    "            v1, original = emotioninvoke(frame)\n",
    "            v2, pose = poseDetector(frame)\n",
    "            v3, eyestat = eyestatus(frame)\n",
    "            fac = frame\n",
    "            t = v1 + v2 + v3\n",
    "            self.ls.append(t)\n",
    "\n",
    "            # Update the average every 5 seconds\n",
    "            if self.curr_time - self.last_time > 5:\n",
    "                avg = sum(self.ls) / len(self.ls)\n",
    "                self.avg_label.setText(f\"Average: {avg:.2f}\")\n",
    "                self.last_time = self.curr_time\n",
    "                self.ls = []\n",
    "\n",
    "            # Convert each filtered frame to RGB and display in corresponding label\n",
    "            filtered_frames = [original, pose, eyestat, fac]\n",
    "            for i, label in enumerate(self.labels):\n",
    "                rgb_frame = cv.cvtColor(filtered_frames[i], cv.COLOR_BGR2RGB)\n",
    "                h, w, _ = rgb_frame.shape\n",
    "                bytes_per_line = 3 * w\n",
    "                qt_img = QImage(rgb_frame.data, w, h, bytes_per_line, QImage.Format_RGB888)\n",
    "                pixmap = QPixmap.fromImage(qt_img)\n",
    "                label.setPixmap(pixmap.scaled(label.width(), label.height(), Qt.KeepAspectRatio))\n",
    "\n",
    "    def stop_video(self):\n",
    "        self.timer.stop()\n",
    "        self.capture.release()\n",
    "        for label in self.labels:\n",
    "            label.clear()\n",
    "        self.avg_label.setText(\"Average: Stopped\")\n",
    "\n",
    "    def closeEvent(self, event):\n",
    "        self.capture.release()\n",
    "\n",
    "class WelcomeScreen(QMainWindow):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.setWindowTitle(\"Welcome Screen\")\n",
    "        self.setGeometry(100, 100, 1200, 800)\n",
    "        # Welcome label\n",
    "        self.label = QLabel(\"Welcome to the Application!\", self)\n",
    "        self.label.setFont(QFont(\"Arial\", 16))\n",
    "        self.label.setAlignment(Qt.AlignCenter)\n",
    "        # Next button\n",
    "        self.next_button = QPushButton(\"Next\", self)\n",
    "        self.next_button.clicked.connect(self.goto_login)\n",
    "\n",
    "        # Layout\n",
    "        layout = QVBoxLayout()\n",
    "        layout.addWidget(self.label)\n",
    "        layout.addWidget(self.next_button)\n",
    "\n",
    "        container = QWidget()\n",
    "        container.setLayout(layout)\n",
    "        self.setCentralWidget(container)\n",
    "\n",
    "    def goto_login(self):\n",
    "        self.login_screen = LoginScreen()\n",
    "        self.login_screen.show()\n",
    "        self.close()\n",
    "\n",
    "\n",
    "class LoginScreen(QMainWindow):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.setWindowTitle(\"Login Screen\")\n",
    "        self.setGeometry(100, 100, 1200, 800)\n",
    "\n",
    "        # Username and password labels\n",
    "        self.username_label = QLabel(\"Username:\", self)\n",
    "        self.username_label.setFont(QFont(\"Arial\", 12))\n",
    "        self.password_label = QLabel(\"Password:\", self)\n",
    "        self.password_label.setFont(QFont(\"Arial\", 12))\n",
    "\n",
    "        # Username and password input fields\n",
    "        self.username_input = QLineEdit(self)\n",
    "        self.password_input = QLineEdit(self)\n",
    "        self.password_input.setEchoMode(QLineEdit.Password)\n",
    "\n",
    "        # Login button\n",
    "        self.login_button = QPushButton(\"Login\", self)\n",
    "        self.login_button.clicked.connect(self.validate_login)\n",
    "\n",
    "        # Layout\n",
    "        layout = QVBoxLayout()\n",
    "        layout.addWidget(self.username_label)\n",
    "        layout.addWidget(self.username_input)\n",
    "        layout.addWidget(self.password_label)\n",
    "        layout.addWidget(self.password_input)\n",
    "        layout.addWidget(self.login_button)\n",
    "\n",
    "        container = QWidget()\n",
    "        container.setLayout(layout)\n",
    "        self.setCentralWidget(container)\n",
    "\n",
    "    def validate_login(self):\n",
    "        username = self.username_input.text()\n",
    "        password = self.password_input.text()\n",
    "\n",
    "        # Basic validation\n",
    "        if username == \"a\" and password == \"p\":  # Example credentials\n",
    "            self.main_screen = MainScreen()\n",
    "            self.main_screen.show()\n",
    "            self.close()\n",
    "        else:\n",
    "            self.username_input.clear()\n",
    "            self.password_input.clear()\n",
    "            self.username_input.setPlaceholderText(\"Invalid credentials, try again!\")\n",
    "\n",
    "\n",
    "class MainScreen(QMainWindow):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.setWindowTitle(\"Main Screen\")\n",
    "        self.setGeometry(100, 100, 1200, 800)\n",
    "\n",
    "        # Web link label\n",
    "        self.link_label = QLabel(\n",
    "            '<a href=\"https://www.example.com\" style=\"color:blue;\">Click here to visit Example.com</a>',\n",
    "            self,\n",
    "        )\n",
    "        self.link_label.setOpenExternalLinks(True)\n",
    "        self.link_label.setAlignment(Qt.AlignCenter)\n",
    "        self.link_label.setFont(QFont(\"Arial\", 12))\n",
    "\n",
    "        # Start button\n",
    "        self.start_button = QPushButton(\"Start\", self)\n",
    "        self.start_button.clicked.connect(self.goto_login)\n",
    "        # Layout\n",
    "        layout = QVBoxLayout()\n",
    "        layout.addWidget(self.link_label)\n",
    "        layout.addWidget(self.start_button)\n",
    "\n",
    "        container = QWidget()\n",
    "        container.setLayout(layout)\n",
    "        self.setCentralWidget(container)\n",
    "\n",
    "    def goto_login(self):\n",
    "        self.login_screen = VideoWindow()\n",
    "        self.login_screen.show()\n",
    "        self.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = QApplication(sys.argv)\n",
    "\n",
    "    # Initialize and show the welcome screen\n",
    "    welcome_screen = WelcomeScreen()\n",
    "    welcome_screen.show()\n",
    "\n",
    "    sys.exit(app.exec_())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
