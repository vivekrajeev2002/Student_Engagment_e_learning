{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2\n",
    "from PyQt5.QtWidgets import QApplication, QWidget, QVBoxLayout, QLabel, QHBoxLayout\n",
    "from PyQt5.QtCore import QTimer, Qt\n",
    "from PyQt5.QtGui import QImage, QPixmap\n",
    "from fer import FER\n",
    "import dlib\n",
    "from scipy.spatial import distance\n",
    "import face_recognition\n",
    "import cv2\n",
    "import pickle\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import math\n",
    "import argparse\n",
    "import time\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from scipy.spatial import distance as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "USER_FACE_WIDTH = 140  # [mm]\n",
    "NOSE_TO_CAMERA_DISTANCE = 600  # [mm]\n",
    "DEFAULT_WEBCAM = 0\n",
    "SHOW_ALL_FEATURES = True\n",
    "ENABLE_HEAD_POSE = True\n",
    "SHOW_ON_SCREEN_DATA = True\n",
    "TOTAL_BLINKS = 0\n",
    "EYES_BLINK_FRAME_COUNTER = 0\n",
    "BLINK_THRESHOLD = 0.51\n",
    "EYE_AR_CONSEC_FRAMES = 2\n",
    "LEFT_EYE_IRIS = [474, 475, 476, 477]\n",
    "RIGHT_EYE_IRIS = [469, 470, 471, 472]\n",
    "LEFT_EYE_OUTER_CORNER = [33]\n",
    "LEFT_EYE_INNER_CORNER = [133]\n",
    "RIGHT_EYE_OUTER_CORNER = [362]\n",
    "RIGHT_EYE_INNER_CORNER = [263]\n",
    "RIGHT_EYE_POINTS = [33, 160, 159, 158, 133, 153, 145, 144]\n",
    "LEFT_EYE_POINTS = [362, 385, 386, 387, 263, 373, 374, 380]\n",
    "NOSE_TIP_INDEX = 4\n",
    "CHIN_INDEX = 152\n",
    "LEFT_EYE_LEFT_CORNER_INDEX = 33\n",
    "RIGHT_EYE_RIGHT_CORNER_INDEX = 263\n",
    "LEFT_MOUTH_CORNER_INDEX = 61\n",
    "RIGHT_MOUTH_CORNER_INDEX = 291\n",
    "MIN_DETECTION_CONFIDENCE = 0.8\n",
    "MIN_TRACKING_CONFIDENCE = 0.8\n",
    "MOVING_AVERAGE_WINDOW = 10\n",
    "initial_pitch, initial_yaw, initial_roll = None, None, None\n",
    "calibrated = False\n",
    "PRINT_DATA = True  # Enable/disable data printing\n",
    "DEFAULT_WEBCAM = 0  # Default webcam number\n",
    "SHOW_ALL_FEATURES = True  # Show all facial landmarks if True\n",
    "SHOW_BLINK_COUNT_ON_SCREEN = True  # Toggle to show the blink count on the video feed\n",
    "TOTAL_BLINKS = 0  # Tracks the total number of blinks detected\n",
    "EYES_BLINK_FRAME_COUNTER = (\n",
    "    0  # Counts the number of consecutive frames with a potential blink\n",
    ")\n",
    "BLINK_THRESHOLD = 0.51  # Threshold for the eye aspect ratio to trigger a blink\n",
    "EYE_AR_CONSEC_FRAMES = (\n",
    "    2  # Number of consecutive frames below the threshold to confirm a blink\n",
    ")\n",
    "\n",
    "\n",
    "# Iris and eye corners landmarks indices\n",
    "LEFT_IRIS = [474, 475, 476, 477]\n",
    "RIGHT_IRIS = [469, 470, 471, 472]\n",
    "L_H_LEFT = [33]  # Left eye Left Corner\n",
    "L_H_RIGHT = [133]  # Left eye Right Corner\n",
    "R_H_LEFT = [362]  # Right eye Left Corner\n",
    "R_H_RIGHT = [263]  # Right eye Right Corner\n",
    "\n",
    "# Blinking Detection landmark's indices.\n",
    "# P0, P3, P4, P5, P8, P11, P12, P13\n",
    "RIGHT_EYE_POINTS = [33, 160, 159, 158, 133, 153, 145, 144]\n",
    "LEFT_EYE_POINTS = [362, 385, 386, 387, 263, 373, 374, 380]\n",
    "\n",
    "# Face Selected points indices for Head Pose Estimation\n",
    "_indices_pose = [1, 33, 61, 199, 263, 291]\n",
    "\n",
    "# Function to calculate vector position\n",
    "def vector_position(point1, point2):\n",
    "    x1, y1 = point1.ravel()\n",
    "    x2, y2 = point2.ravel()\n",
    "    return x2 - x1, y2 - y1\n",
    "\n",
    "\n",
    "def euclidean_distance_3D(points):\n",
    "    \"\"\"Calculates the Euclidean distance between two points in 3D space.\n",
    "\n",
    "    Args:\n",
    "        points: A list of 3D points.\n",
    "\n",
    "    Returns:\n",
    "        The Euclidean distance between the two points.\n",
    "\n",
    "        # Comment: This function calculates the Euclidean distance between two points in 3D space.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the three points.\n",
    "    P0, P3, P4, P5, P8, P11, P12, P13 = points\n",
    "\n",
    "    # Calculate the numerator.\n",
    "    numerator = (\n",
    "        np.linalg.norm(P3 - P13) ** 3\n",
    "        + np.linalg.norm(P4 - P12) ** 3\n",
    "        + np.linalg.norm(P5 - P11) ** 3\n",
    "    )\n",
    "\n",
    "    # Calculate the denominator.\n",
    "    denominator = 3 * np.linalg.norm(P0 - P8) ** 3\n",
    "\n",
    "    # Calculate the distance.\n",
    "    distance = numerator / denominator\n",
    "\n",
    "    return distance\n",
    "\n",
    "def estimate_head_pose(landmarks, image_size):\n",
    "    # Scale factor based on user's face width (assumes model face width is 150mm)\n",
    "    scale_factor = USER_FACE_WIDTH / 150.0\n",
    "    # 3D model points.\n",
    "    model_points = np.array([\n",
    "        (0.0, 0.0, 0.0),             # Nose tip\n",
    "        (0.0, -330.0 * scale_factor, -65.0 * scale_factor),        # Chin\n",
    "        (-225.0 * scale_factor, 170.0 * scale_factor, -135.0 * scale_factor),     # Left eye left corner\n",
    "        (225.0 * scale_factor, 170.0 * scale_factor, -135.0 * scale_factor),      # Right eye right corner\n",
    "        (-150.0 * scale_factor, -150.0 * scale_factor, -125.0 * scale_factor),    # Left Mouth corner\n",
    "        (150.0 * scale_factor, -150.0 * scale_factor, -125.0 * scale_factor)      # Right mouth corner\n",
    "    ])\n",
    "    \n",
    "\n",
    "    # Camera internals\n",
    "    focal_length = image_size[1]\n",
    "    center = (image_size[1]/2, image_size[0]/2)\n",
    "    camera_matrix = np.array(\n",
    "        [[focal_length, 0, center[0]],\n",
    "         [0, focal_length, center[1]],\n",
    "         [0, 0, 1]], dtype = \"double\"\n",
    "    )\n",
    "\n",
    "    # Assuming no lens distortion\n",
    "    dist_coeffs = np.zeros((4,1))\n",
    "\n",
    "    # 2D image points from landmarks, using defined indices\n",
    "    image_points = np.array([\n",
    "        landmarks[NOSE_TIP_INDEX],            # Nose tip\n",
    "        landmarks[CHIN_INDEX],                # Chin\n",
    "        landmarks[LEFT_EYE_LEFT_CORNER_INDEX],  # Left eye left corner\n",
    "        landmarks[RIGHT_EYE_RIGHT_CORNER_INDEX],  # Right eye right corner\n",
    "        landmarks[LEFT_MOUTH_CORNER_INDEX],      # Left mouth corner\n",
    "        landmarks[RIGHT_MOUTH_CORNER_INDEX]      # Right mouth corner\n",
    "    ], dtype=\"double\")\n",
    "\n",
    "\n",
    "        # Solve for pose\n",
    "    (success, rotation_vector, translation_vector) = cv.solvePnP(model_points, image_points, camera_matrix, dist_coeffs, flags=cv.SOLVEPNP_ITERATIVE)\n",
    "\n",
    "    # Convert rotation vector to rotation matrix\n",
    "    rotation_matrix, _ = cv.Rodrigues(rotation_vector)\n",
    "\n",
    "    # Combine rotation matrix and translation vector to form a 3x4 projection matrix\n",
    "    projection_matrix = np.hstack((rotation_matrix, translation_vector.reshape(-1, 1)))\n",
    "\n",
    "    # Decompose the projection matrix to extract Euler angles\n",
    "    _, _, _, _, _, _, euler_angles = cv.decomposeProjectionMatrix(projection_matrix)\n",
    "    pitch, yaw, roll = euler_angles.flatten()[:3]\n",
    "\n",
    "\n",
    "     # Normalize the pitch angle\n",
    "    pitch = normalize_pitch(pitch)\n",
    "\n",
    "    return pitch, yaw, roll\n",
    "\n",
    "def normalize_pitch(pitch):\n",
    "    \"\"\"\n",
    "    Normalize the pitch angle to be within the range of [-90, 90].\n",
    "\n",
    "    Args:\n",
    "        pitch (float): The raw pitch angle in degrees.\n",
    "\n",
    "    Returns:\n",
    "        float: The normalized pitch angle.\n",
    "    \"\"\"\n",
    "    # Map the pitch angle to the range [-180, 180]\n",
    "    if pitch > 180:\n",
    "        pitch -= 360\n",
    "\n",
    "    # Invert the pitch angle for intuitive up/down movement\n",
    "    pitch = -pitch\n",
    "\n",
    "    # Ensure that the pitch is within the range of [-90, 90]\n",
    "    if pitch < -90:\n",
    "        pitch = -(180 + pitch)\n",
    "    elif pitch > 90:\n",
    "        pitch = 180 - pitch\n",
    "        \n",
    "    pitch = -pitch\n",
    "\n",
    "    return pitch\n",
    "\n",
    "# This function calculates the blinking ratio of a person.\n",
    "def blinking_ratio(landmarks):\n",
    "    \"\"\"Calculates the blinking ratio of a person.\n",
    "\n",
    "    Args:\n",
    "        landmarks: A facial landmarks in 3D normalized.\n",
    "\n",
    "    Returns:\n",
    "        The blinking ratio of the person, between 0 and 1, where 0 is fully open and 1 is fully closed.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the right eye ratio.\n",
    "    right_eye_ratio = euclidean_distance_3D(landmarks[RIGHT_EYE_POINTS])\n",
    "\n",
    "    # Get the left eye ratio.\n",
    "    left_eye_ratio = euclidean_distance_3D(landmarks[LEFT_EYE_POINTS])\n",
    "\n",
    "    # Calculate the blinking ratio.\n",
    "    ratio = (right_eye_ratio + left_eye_ratio + 1) / 2\n",
    "\n",
    "    return ratio\n",
    "\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh.FaceMesh(\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=MIN_DETECTION_CONFIDENCE,\n",
    "    min_tracking_confidence=MIN_TRACKING_CONFIDENCE,\n",
    ")\n",
    "\n",
    "def poseDetector(out):\n",
    "    v=0\n",
    "    ENABLE_HEAD_POSE = True\n",
    "    SHOW_ON_SCREEN_DATA = True\n",
    "    TOTAL_BLINKS = 0\n",
    "    EYES_BLINK_FRAME_COUNTER = 0\n",
    "    BLINK_THRESHOLD = 0.51\n",
    "    EYE_AR_CONSEC_FRAMES = 2\n",
    "    LEFT_EYE_IRIS = [474, 475, 476, 477]\n",
    "    RIGHT_EYE_IRIS = [469, 470, 471, 472]\n",
    "    LEFT_EYE_OUTER_CORNER = [33]\n",
    "    LEFT_EYE_INNER_CORNER = [133]\n",
    "    RIGHT_EYE_OUTER_CORNER = [362]\n",
    "    RIGHT_EYE_INNER_CORNER = [263]\n",
    "    EYES_BLINK_FRAME_COUNTER = (\n",
    "        0  # Counts the number of consecutive frames with a potential blink\n",
    "    )\n",
    "    BLINK_THRESHOLD = 0.51  # Threshold for the eye aspect ratio to trigger a blink\n",
    "    EYE_AR_CONSEC_FRAMES = (\n",
    "        2  # Number of consecutive frames below the threshold to confirm a blink\n",
    "    )\n",
    "    frame = out.copy()         \n",
    "    rgb_frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "    img_h, img_w = frame.shape[:2]\n",
    "    results = mp_face_mesh.process(rgb_frame)\n",
    "    if results.multi_face_landmarks:\n",
    "        mesh_points = np.array(\n",
    "            [\n",
    "                np.multiply([p.x, p.y], [img_w, img_h]).astype(int)\n",
    "                for p in results.multi_face_landmarks[0].landmark\n",
    "            ]\n",
    "        )\n",
    "        mesh_points_3D = np.array(\n",
    "            [[n.x, n.y, n.z] for n in results.multi_face_landmarks[0].landmark]\n",
    "        )\n",
    "        # getting the head pose estimation 3d points\n",
    "        head_pose_points_3D = np.multiply(\n",
    "            mesh_points_3D[_indices_pose], [img_w, img_h, 1]\n",
    "        )\n",
    "        head_pose_points_2D = mesh_points[_indices_pose]\n",
    "\n",
    "        # collect nose three dimension and two dimension points\n",
    "        nose_3D_point = np.multiply(head_pose_points_3D[0], [1, 1, 3000])\n",
    "        nose_2D_point = head_pose_points_2D[0]\n",
    "\n",
    "        # create the camera matrix\n",
    "        focal_length = 1 * img_w\n",
    "\n",
    "        cam_matrix = np.array(\n",
    "            [[focal_length, 0, img_h / 2], [0, focal_length, img_w / 2], [0, 0, 1]]\n",
    "        )\n",
    "\n",
    "        # The distortion parameters\n",
    "        dist_matrix = np.zeros((4, 1), dtype=np.float64)\n",
    "\n",
    "        head_pose_points_2D = np.delete(head_pose_points_3D, 2, axis=1)\n",
    "        head_pose_points_3D = head_pose_points_3D.astype(np.float64)\n",
    "        head_pose_points_2D = head_pose_points_2D.astype(np.float64)\n",
    "        # Solve PnP\n",
    "        success, rot_vec, trans_vec = cv.solvePnP(\n",
    "            head_pose_points_3D, head_pose_points_2D, cam_matrix, dist_matrix\n",
    "        )\n",
    "        # Get rotational matrix\n",
    "        rotation_matrix, jac = cv.Rodrigues(rot_vec)\n",
    "\n",
    "        # Get angles\n",
    "        angles, mtxR, mtxQ, Qx, Qy, Qz = cv.RQDecomp3x3(rotation_matrix)\n",
    "\n",
    "        # Get the y rotation degree\n",
    "        angle_x = angles[0] * 360\n",
    "        angle_y = angles[1] * 360\n",
    "        z = angles[2] * 360\n",
    "\n",
    "        # if angle cross the values then\n",
    "        threshold_angle = 10\n",
    "        # See where the user's head tilting\n",
    "        if angle_y < -threshold_angle:\n",
    "            face_looks = \"Right\"\n",
    "        elif angle_y > threshold_angle:\n",
    "            face_looks = \"Left\"\n",
    "        elif angle_x < -threshold_angle:\n",
    "            face_looks = \"Down\"\n",
    "        elif angle_x > threshold_angle:\n",
    "            face_looks = \"Up\"\n",
    "        else:\n",
    "            face_looks = \"Forward\"\n",
    "            v=1\n",
    "        if face_looks == 'Forward':\n",
    "            face_looks='Listening '\n",
    "        else:\n",
    "            face_looks=\"Not Listening  (\"+face_looks+')'\n",
    "        if SHOW_ON_SCREEN_DATA:\n",
    "            cv.putText(\n",
    "                frame,\n",
    "                f\"Engagment :  {face_looks}\",\n",
    "                (img_w - 400, 80),\n",
    "                cv.FONT_HERSHEY_TRIPLEX,\n",
    "                0.6,\n",
    "                (0, 0, 0),\n",
    "                2,\n",
    "                cv.LINE_AA,\n",
    "            )\n",
    "        # Display the nose direction\n",
    "        nose_3d_projection, jacobian = cv.projectPoints(nose_3D_point, rot_vec, trans_vec, cam_matrix, dist_matrix)\n",
    "\n",
    "        p1 = nose_2D_point\n",
    "        p2 = (\n",
    "            int(nose_2D_point[0] + angle_y * 10),\n",
    "            int(nose_2D_point[1] - angle_x * 10),\n",
    "        )\n",
    "\n",
    "        cv.line(frame, p1, p2, (255, 0, 255), 3)\n",
    "        # getting the blinking ratio\n",
    "        eyes_aspect_ratio = blinking_ratio(mesh_points_3D)\n",
    "        \n",
    "        if eyes_aspect_ratio <= BLINK_THRESHOLD:\n",
    "            EYES_BLINK_FRAME_COUNTER += 1\n",
    "        # else check if eyes are closed is greater EYE_AR_CONSEC_FRAMES frame then\n",
    "        # count the this as a blink\n",
    "        # make frame counter equal to zero\n",
    "\n",
    "        else:\n",
    "            if EYES_BLINK_FRAME_COUNTER > EYE_AR_CONSEC_FRAMES:\n",
    "                TOTAL_BLINKS += 1\n",
    "            EYES_BLINK_FRAME_COUNTER = 0\n",
    "        \n",
    "        # Display all facial landmarks if enabled\n",
    "\n",
    "        for point in mesh_points:\n",
    "            cv.circle(frame, tuple(point), 1, (0, 255, 0), -1)\n",
    "        # Process and display eye features\n",
    "        \n",
    "        (l_cx, l_cy), l_radius = cv.minEnclosingCircle(mesh_points[LEFT_EYE_IRIS])\n",
    "        (r_cx, r_cy), r_radius = cv.minEnclosingCircle(mesh_points[RIGHT_EYE_IRIS])\n",
    "        center_left = np.array([l_cx, l_cy], dtype=np.int32)\n",
    "        center_right = np.array([r_cx, r_cy], dtype=np.int32)\n",
    "        \n",
    "        # Highlighting the irises and corners of the eyes\n",
    "        cv.circle(\n",
    "            frame, center_left, int(l_radius), (255, 0, 255), 2, cv.LINE_AA\n",
    "        )  # Left iris\n",
    "        cv.circle(\n",
    "            frame, center_right, int(r_radius), (255, 0, 255), 2, cv.LINE_AA\n",
    "        )  # Right iris\n",
    "        cv.circle(\n",
    "            frame, mesh_points[LEFT_EYE_INNER_CORNER][0], 3, (255, 255, 255), -1, cv.LINE_AA\n",
    "        )  # Left eye right corner\n",
    "        cv.circle(\n",
    "            frame, mesh_points[LEFT_EYE_OUTER_CORNER][0], 3, (0, 255, 255), -1, cv.LINE_AA\n",
    "        )  # Left eye left corner\n",
    "        cv.circle(\n",
    "            frame, mesh_points[RIGHT_EYE_INNER_CORNER][0], 3, (255, 255, 255), -1, cv.LINE_AA\n",
    "        )  # Right eye right corner\n",
    "        cv.circle(\n",
    "            frame, mesh_points[RIGHT_EYE_OUTER_CORNER][0], 3, (0, 255, 255), -1, cv.LINE_AA\n",
    "        )  # Right eye left corner\n",
    "        \n",
    "        # Calculating relative positions\n",
    "        l_dx, l_dy = vector_position(mesh_points[LEFT_EYE_OUTER_CORNER], center_left)\n",
    "        r_dx, r_dy = vector_position(mesh_points[RIGHT_EYE_OUTER_CORNER], center_right)\n",
    "\n",
    "        \n",
    "    # Writing the on screen data on the frame\n",
    "    \n",
    "        cv.putText(frame, f\"Blinks: {TOTAL_BLINKS}\", (30, 80), cv.FONT_HERSHEY_DUPLEX, 0.8, (0, 255, 0), 2, cv.LINE_AA)\n",
    "        if ENABLE_HEAD_POSE:\n",
    "            #cv.putText(frame, f\"Pitch: {int(pitch)}\", (30, 110), cv.FONT_HERSHEY_DUPLEX, 0.8, (0, 255, 0), 2, cv.LINE_AA)\n",
    "            cv.putText(frame, f\"Blinks: {TOTAL_BLINKS}\", (30, 80), cv.FONT_HERSHEY_DUPLEX, 0.8, (0, 255, 0), 2, cv.LINE_AA)\n",
    "            #cv.putText(out, f\"Roll: {int(roll)}\", (30, 170), cv.FONT_HERSHEY_DUPLEX, 0.8, (0, 255, 0), 2, cv.LINE_AA)\n",
    "        return v,frame\n",
    "    else:\n",
    "        return v,out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:c:\\Users\\vivek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6\n",
      "3.723529411764706\n",
      "4.281818181818182\n",
      "4.191304347826088\n",
      "3.973913043478261\n",
      "3.5913043478260875\n",
      "3.1608695652173915\n",
      "4.465217391304348\n",
      "4.658064516129032\n",
      "4.275\n",
      "3.865217391304348\n",
      "3.6\n",
      "4.291304347826087\n",
      "4.5\n",
      "3.2695652173913046\n",
      "2.4\n",
      "4.791304347826087\n",
      "4.356521739130435\n",
      "4.2956521739130435\n",
      "4.6521739130434785\n",
      "3.9499999999999997\n",
      "4.595652173913043\n",
      "3.9217391304347826\n",
      "3.1608695652173915\n",
      "3.8782608695652177\n",
      "3.5923076923076924\n",
      "2.208695652173913\n",
      "3.2217391304347824\n",
      "3.545833333333334\n",
      "3.6260869565217395\n",
      "2.1870967741935483\n",
      "3.465217391304348\n",
      "2.926086956521739\n",
      "4.077272727272727\n",
      "4.5551724137931044\n",
      "4.4270270270270276\n",
      "4.573913043478261\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:C:\\Users\\vivek\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"registered_face_encoding.pkl\", \"rb\") as f:\n",
    "    registered_face_encoding = pickle.load(f)\n",
    "#FACIAL RECOGNIITON CODE\n",
    "def face(out):\n",
    "    frame=out.copy()\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Find all face locations and encodings in the current frame\n",
    "    face_locations = face_recognition.face_locations(rgb_frame)\n",
    "    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "\n",
    "    # Loop through all detected faces\n",
    "    for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):\n",
    "        # Compare the current face encoding with the registered face encoding\n",
    "        matches = face_recognition.compare_faces([registered_face_encoding], face_encoding)\n",
    "\n",
    "        if True in matches:\n",
    "            # If the face matches, label it as recognized\n",
    "            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, \"Vivek Rajeev V\", (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "        else:\n",
    "            # If the face doesn't match, label it as unknown\n",
    "            cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "            cv2.putText(frame, \"Unknown\", (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "    return frame\n",
    "\n",
    "#CALCULATE EYE STATUS\n",
    "def eye_aspect_ratio(eye):\n",
    "    # Compute the Euclidean distances between the two sets of vertical eye landmarks\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "\n",
    "    # Compute the Euclidean distance between the horizontal eye landmarks\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "\n",
    "    # Calculate the eye aspect ratio\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Load pre-trained facial landmarks predictor and detector\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")  # Download from dlib website\n",
    "\n",
    "# Indices for eyes landmarks in the 68-point model\n",
    "LEFT_EYE = slice(36, 42)\n",
    "RIGHT_EYE = slice(42, 48)\n",
    "\n",
    "# Threshold for EAR to detect closure\n",
    "EAR_THRESHOLD = 0.25\n",
    "\n",
    "def eyestatus(out):\n",
    "    frame=out.copy()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # Detect faces\n",
    "    faces = detector(gray)\n",
    "    v1=0\n",
    "    for face in faces:\n",
    "        # Detect landmarks\n",
    "        landmarks = predictor(gray, face)\n",
    "\n",
    "        # Convert landmarks to a numpy array\n",
    "        landmarks_points = [(landmarks.part(n).x, landmarks.part(n).y) for n in range(68)]\n",
    "\n",
    "        # Extract left and right eye points\n",
    "        left_eye = landmarks_points[LEFT_EYE]\n",
    "        right_eye = landmarks_points[RIGHT_EYE]\n",
    "\n",
    "        # Calculate EAR for both eyes\n",
    "        left_ear = eye_aspect_ratio(left_eye)\n",
    "        right_ear = eye_aspect_ratio(right_eye)\n",
    "\n",
    "        # Average the EAR\n",
    "        avg_ear = (left_ear + right_ear) / 2.0\n",
    "        \n",
    "        # Detect eye closure\n",
    "        if avg_ear < EAR_THRESHOLD:\n",
    "            v1=0\n",
    "            cv2.putText(frame, \"Eyes Closed\", (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        else:\n",
    "            v1=2.5\n",
    "            cv2.putText(frame, \"Eyes Open\", (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        # Draw eyes on the frame for visualization\n",
    "        for point in left_eye + right_eye:\n",
    "            cv2.circle(frame, point, 2, (0, 255, 0), -1)\n",
    "    \n",
    "    return v1,frame\n",
    "\n",
    "\n",
    "emotion_detector = FER()\n",
    "def emotioninvoke(fr):\n",
    "    v=0\n",
    "    frame=fr.copy()\n",
    "    emotions = emotion_detector.detect_emotions(frame)\n",
    "    if(len(emotions)==0):\n",
    "        return v,frame\n",
    "    # Loop through detected faces and their emotions\n",
    "    \n",
    "    for emotion_data in emotions:\n",
    "        box = emotion_data[\"box\"]  # Bounding box for the face\n",
    "        emotions = emotion_data[\"emotions\"]  # Dictionary of emotions\n",
    "\n",
    "        # Get the most likely emotion\n",
    "        top_emotion = max(emotions, key=emotions.get)\n",
    "\n",
    "        # Draw the bounding box\n",
    "        x, y, w, h = box\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 255), 2)\n",
    "        \n",
    "        # Display the top emotion\n",
    "        label = f\"{top_emotion}: {emotions[top_emotion]:.2f}\"\n",
    "        cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "        if(label[0] == 'a'):\n",
    "            v=0.1\n",
    "        elif(label[0:2] == 'sa'):\n",
    "            v=0.3\n",
    "        elif(label[0] == 'f'):\n",
    "            v=0.5\n",
    "        elif(label[0] == 's'):\n",
    "            v=0.7\n",
    "        elif(label[0] == 'd'):\n",
    "            v=0.9\n",
    "        elif(label[0] == 'h'):\n",
    "            v=1.1\n",
    "        elif(label[0] == 'n'):\n",
    "            v = 1.4\n",
    "            \n",
    "        return v,frame\n",
    "\n",
    "from PyQt5.QtWidgets import (\n",
    "    QApplication, QWidget, QHBoxLayout, QVBoxLayout, QLabel, QPushButton\n",
    ")\n",
    "from PyQt5.QtCore import QTimer, Qt\n",
    "from PyQt5.QtGui import QImage, QPixmap\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "class VideoWindow(QWidget):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ls = []\n",
    "        self.curr_time = time.time()\n",
    "        self.last_time = time.time()\n",
    "        self.setWindowTitle(\"OpenCV Output with Filters\")\n",
    "        self.setGeometry(100, 100, 1200, 800)\n",
    "        self.setStyleSheet(\"background-color: purple;\")  # Set purple background color\n",
    "\n",
    "        # Layout to hold four labels in a row\n",
    "        self.video_layout = QHBoxLayout()\n",
    "        self.labels = [QLabel() for _ in range(4)]\n",
    "        for label in self.labels:\n",
    "            label.setFixedSize(400, 370)  # Increase the size of video frames\n",
    "            label.setStyleSheet(\"background-color: black;\")  # Add background for labels\n",
    "            self.video_layout.addWidget(label)\n",
    "\n",
    "        # Label to display the average of `ls`\n",
    "        self.avg_label = QLabel(\"Average: Calculating...\")\n",
    "        self.avg_label.setAlignment(Qt.AlignCenter)\n",
    "        self.avg_label.setStyleSheet(\n",
    "            \"font-size: 18px; font-weight: bold; color: white; background-color: navy; padding: 10px;\"\n",
    "        )\n",
    "\n",
    "        # Stop button\n",
    "        self.stop_button = QPushButton(\"Stop\")\n",
    "        self.stop_button.setStyleSheet(\n",
    "            \"font-size: 18px; padding: 10px; background-color: red; color: white;\"\n",
    "        )\n",
    "        self.stop_button.clicked.connect(self.stop_video)\n",
    "\n",
    "        # Main layout\n",
    "        self.main_layout = QVBoxLayout()\n",
    "        self.main_layout.addLayout(self.video_layout)\n",
    "        self.main_layout.addWidget(self.avg_label, alignment=Qt.AlignCenter)\n",
    "        self.main_layout.addWidget(self.stop_button, alignment=Qt.AlignCenter)\n",
    "        self.setLayout(self.main_layout)\n",
    "\n",
    "        # OpenCV capture (video feed)\n",
    "        self.capture = cv2.VideoCapture(0)\n",
    "\n",
    "        # Timer to periodically update the feed\n",
    "        self.timer = QTimer(self)\n",
    "        self.timer.timeout.connect(self.update_video_stream)\n",
    "        self.timer.start(100)\n",
    "\n",
    "    def update_video_stream(self):\n",
    "        ret, frame = self.capture.read()\n",
    "        if ret:\n",
    "            self.curr_time = time.time()\n",
    "\n",
    "            v1, original = emotioninvoke(frame)\n",
    "            v2, pose = poseDetector(frame)\n",
    "            v3, eyestat = eyestatus(frame)\n",
    "            fac = frame\n",
    "            t = v1 + v2 + v3\n",
    "            self.ls.append(t)\n",
    "\n",
    "            # Update the average every 5 seconds\n",
    "            if self.curr_time - self.last_time > 5:\n",
    "                avg = sum(self.ls) / len(self.ls)\n",
    "                self.avg_label.setText(f\"Average: {avg:.2f}\")\n",
    "                self.last_time = self.curr_time\n",
    "                self.ls = []\n",
    "\n",
    "            # Convert each filtered frame to RGB and display in corresponding label\n",
    "            filtered_frames = [original, pose, eyestat, fac]\n",
    "            for i, label in enumerate(self.labels):\n",
    "                rgb_frame = cv2.cvtColor(filtered_frames[i], cv2.COLOR_BGR2RGB)\n",
    "                h, w, _ = rgb_frame.shape\n",
    "                bytes_per_line = 3 * w\n",
    "                qt_img = QImage(rgb_frame.data, w, h, bytes_per_line, QImage.Format_RGB888)\n",
    "                pixmap = QPixmap.fromImage(qt_img)\n",
    "                label.setPixmap(pixmap.scaled(label.width(), label.height(), Qt.KeepAspectRatio))\n",
    "\n",
    "    def stop_video(self):\n",
    "        self.timer.stop()\n",
    "        self.capture.release()\n",
    "        for label in self.labels:\n",
    "            label.clear()\n",
    "        self.avg_label.setText(\"Average: Stopped\")\n",
    "\n",
    "    def closeEvent(self, event):\n",
    "        self.capture.release()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app = QApplication(sys.argv)\n",
    "\n",
    "    window = VideoWindow()\n",
    "    window.show()\n",
    "    sys.exit(app.exec_())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
